{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from td_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data synthesis: Creating a Speech Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Audio Recordings to Spectrograms\n",
    "\n",
    "What really is an audio recording? \n",
    "* A microphone records little variations in air pressure over time, and it is these little variations in air pressure that your ear also perceives as sound. \n",
    "* You can think of an audio recording as a long list of numbers measuring the little air pressure changes detected by the microphone. \n",
    "* We will use audio sampled at 44100 Hz (or 44100 Hertz). \n",
    "    * This means the microphone gives us 44,100 numbers per second. \n",
    "    * Thus, a 10 second audio clip is represented by 441,000 numbers (= $10 \\times 44,100$). \n",
    "\n",
    "#### Spectrogram\n",
    "* It is quite difficult to figure out from this \"raw\" representation of audio whether the word \"activate\" was said. \n",
    "* In  order to help your sequence model more easily learn to detect trigger words, we will compute a *spectrogram* of the audio. \n",
    "* The spectrogram tells us how much different frequencies are present in an audio clip at any moment in time. \n",
    "* If you've ever taken an advanced class on signal processing or on Fourier transforms:\n",
    "    * A spectrogram is computed by sliding a window over the raw audio signal, and calculating the most active frequencies in each window using a Fourier transform. \n",
    "\n",
    "Let's look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_spectrogram(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing into time-intervals\n",
    "Note that we may divide a 10 second interval of time with different units (steps).\n",
    "* Raw audio divides 10 seconds into 441,000 units.\n",
    "* A spectrogram divides 10 seconds into 5,511 units.\n",
    "    * $T_x = 5511$\n",
    "* You will use a Python module `pydub` to synthesize audio, and it divides 10 seconds into 10,000 units.\n",
    "* The output of our model will divide 10 seconds into 1,375 units.\n",
    "    * $T_y = 1375$\n",
    "    * For each of the 1375 time steps, the model predicts whether someone recently finished saying the trigger word \"activate\". \n",
    "* All of these are hyperparameters and can be changed (except the 441000, which is a function of the microphone). \n",
    "* We have chosen values that are within the standard range used for speech systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a Single Training Example\n",
    "\n",
    "#### Benefits of synthesizing data\n",
    "Because speech data is hard to acquire and label, you can synthesize your training data using the audio clips of activates, negatives, and backgrounds. \n",
    "* It is quite slow to record lots of 10 second audio clips with random \"activates\" in it. \n",
    "* Instead, it is easier to record lots of positives and negative words, and record background noise separately (or download background noise from free online sources). \n",
    "\n",
    "#### Process for Synthesizing an audio clip\n",
    "* To synthesize a single training example, you can:\n",
    "    - Pick a random 10 second background audio clip\n",
    "    - Randomly insert 0-4 audio clips of \"activate\" into this 10 sec. clip\n",
    "    - Randomly insert 0-2 audio clips of negative words into this 10 sec. clip\n",
    "* Because you had synthesized the word \"activate\" into the background clip, you know exactly when in the 10 second clip the \"activate\" makes its appearance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio('./raw_data/')\n",
    "\n",
    "print(\"background len should be 10,000, since it is a 10 sec clip\\n\" + str(len(backgrounds[0])),\"\\n\")\n",
    "print(\"activate[0] len may be around 1000, since an `activate` audio clip is usually around 1 second (but varies a lot) \\n\" + str(len(activates[0])),\"\\n\")\n",
    "print(\"activate[1] len: different `activate` clips can have different lengths\\n\" + str(len(activates[1])),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
    "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "    \n",
    "    Returns:\n",
    "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start, segment_end = segment_time\n",
    "    \n",
    "    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\n",
    "    overlap = False\n",
    "    \n",
    "    # Step 2: loop over the previous_segments start and end times.\n",
    "    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\n",
    "    for previous_start, previous_end in previous_segments: # @KEEP\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "            break\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
    "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
    "print(\"Overlap 1 = \", overlap1)\n",
    "print(\"Overlap 2 = \", overlap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.  \n",
    "    audio_clip -- the audio clip to be inserted/overlaid. \n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "    \n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "    \n",
    "    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \n",
    "    # the new audio clip. (≈ 1 line)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    \n",
    "    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \n",
    "    # picking new segment_time at random until it doesn't overlap. To avoid an endless loop\n",
    "    # we retry 5 times(≈ 2 lines)\n",
    "    retry = 5 # @KEEP \n",
    "    while is_overlapping(segment_time, previous_segments) and retry >= 0:\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "        retry = retry - 1\n",
    "    if not is_overlapping(segment_time, previous_segments):\n",
    "        # Step 3: Append the new segment_time to the list of previous_segments (≈ 1 line)\n",
    "        previous_segments.append(segment_time)\n",
    "        # Step 4: Superpose audio segment and background\n",
    "        new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    else:\n",
    "        new_background = background\n",
    "        segment_time = (10000, 10000)\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time)\n",
    "IPython.display.Audio(\"insert_test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected audio\n",
    "IPython.display.Audio(\"audio_examples/insert_reference.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \n",
    "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
    "    50 following labels should be ones.\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
    "    segment_end_ms -- the end time of the segment in ms\n",
    "    \n",
    "    Returns:\n",
    "    y -- updated labels\n",
    "    \"\"\"\n",
    "    _, Ty = y.shape\n",
    "    \n",
    "    # duration of the background (in terms of spectrogram time-steps)\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    \n",
    "    if segment_end_y < Ty:\n",
    "        # Add 1 to the correct index in the background label (y)\n",
    "        for i in range(segment_end_y + 1, min(segment_end_y + 51, Ty)):\n",
    "            if segment_end_y < Ty:\n",
    "                y[0, i] = 1\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
    "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
    "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(background, activates, negatives, Ty):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording\n",
    "    activates -- a list of audio segments of the word \"activate\"\n",
    "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
    "    Ty -- The number of time steps in the output\n",
    "\n",
    "    Returns:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n",
    "    y = np.zeros((1, Ty))\n",
    "\n",
    "    # Step 2: Initialize segment times as empty list (≈ 1 line)\n",
    "    previous_segments = []\n",
    "    \n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "    \n",
    "    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n",
    "    for random_activate in random_activates: # @KEEP\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end =  segment_time\n",
    "        # Insert labels in \"y\" at segment_end\n",
    "        y = insert_ones(y, segment_end)\n",
    "\n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    # Step 4: Loop over randomly selected negative clips and insert in background\n",
    "    for random_negative in random_negatives: # @KEEP\n",
    "        # Insert the audio clip on the background \n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    \n",
    "    # Standardize the volume of the audio clip \n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    # Export new training example \n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    \n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(18)\n",
    "x, y = create_training_example(backgrounds[0], activates, negatives, Ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can listen to the training example you created and compare it to the spectrogram generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/train_reference.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4543)\n",
    "nsamples = 32\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, nsamples):\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    x, y = create_training_example(backgrounds[i % 2], activates, negatives, Ty)\n",
    "    X.append(x.swapaxes(0,1))\n",
    "    Y.append(y.swapaxes(0,1))\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for further uses\n",
    "# np.save(f'./XY_train/X.npy', X)\n",
    "# np.save(f'./XY_train/Y.npy', Y)\n",
    "# Load the preprocessed training examples\n",
    "# X = np.load(\"./XY_train/X.npy\")\n",
    "# Y = np.load(\"./XY_train/Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: modelf\n",
    "\n",
    "def modelf(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    \n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    # Add a Conv1D with 196 units, kernel size of 15 and stride of 4\n",
    "    X = Conv1D(filters=196,kernel_size=15,strides=4)(X_input)\n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # ReLu activation\n",
    "    X = Activation(\"relu\")(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)                                  \n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X =  GRU(units=128, return_sequences = True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)                                  \n",
    "    # Batch normalization.\n",
    "    X = BatchNormalization()(X)                         \n",
    "    \n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X = GRU(units=128, return_sequences = True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)       \n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X) \n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)                                \n",
    "    \n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    # TimeDistributed  with sigmoid activation \n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n",
    "\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelf_test(target):\n",
    "    Tx = 5511\n",
    "    n_freq = 101\n",
    "    model = target(input_shape = (Tx, n_freq))\n",
    "    expected_model = [['InputLayer', [(None, 5511, 101)], 0],\n",
    "                     ['Conv1D', (None, 1375, 196), 297136, 'valid', 'linear', (4,), (15,), 'GlorotUniform'],\n",
    "                     ['BatchNormalization', (None, 1375, 196), 784],\n",
    "                     ['Activation', (None, 1375, 196), 0],\n",
    "                     ['Dropout', (None, 1375, 196), 0, 0.8],\n",
    "                     ['GRU', (None, 1375, 128), 125184, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['GRU', (None, 1375, 128), 99072, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['TimeDistributed', (None, 1375, 1), 129, 'sigmoid']]\n",
    "    comparator(summary(model), expected_model)\n",
    "    \n",
    "    \n",
    "modelf_test(modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelf(input_shape = (Tx, n_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the model summary to keep track of the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "json_file = open('./models/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('./models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to fine-tune a pretrained model, it is important that you block the weights of all your batchnormalization layers. If you are going to train a new model from scratch, skip the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False\n",
    "model.layers[7].trainable = False\n",
    "model.layers[10].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for two epochs and with a small training set of 32 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-6, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size = 16, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Finally, let's see how your model performs on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Making Predictions\n",
    "<!--\n",
    "can use your model to make predictions on new audio clips.\n",
    "\n",
    "You will first need to compute the predictions for an input audio clip.\n",
    "\n",
    "**Exercise**: Implement predict_activates(). You will need to do the following:\n",
    "\n",
    "1. Compute the spectrogram for the audio file\n",
    "2. Use `np.swap` and `np.expand_dims` to reshape your input to size (1, Tx, n_freqs)\n",
    "5. Use forward propagation on your model to compute the prediction at each output step\n",
    "!-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    # Correct the amplitude of the input file before prediction \n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    audio_clip = match_target_amplitude(audio_clip, -20.0)\n",
    "    file_handle = audio_clip.export(\"tmp.wav\", format=\"wav\")\n",
    "    filename = \"tmp.wav\"\n",
    "\n",
    "    x = graph_spectrogram(filename)\n",
    "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_file = \"audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    i = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    while i < Ty:\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold for 20 consecutive output steps have passed\n",
    "        if consecutive_timesteps > 20:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds) * 1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "            i = 75 * (i // 75 + 1)\n",
    "            continue\n",
    "        # if amplitude is smaller than the threshold reset the consecutive_timesteps counter\n",
    "        if predictions[0, i, 0] < threshold:\n",
    "            consecutive_timesteps = 0\n",
    "        i += 1\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
